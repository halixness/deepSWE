{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# %%\n",
    "import os\n",
    "from datetime import datetime\n",
    "from utils.data_lightning import preloading\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mat\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch as th\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "from models.ae import seq2seq_ConvLSTM\n",
    "import models\n",
    "\n",
    "import argparse\n",
    "import pytorch_ssim\n",
    "import time\n",
    "\n",
    "mat.use(\"Agg\") # headless mode\n",
    "#mat.rcParams['text.color'] = 'w'\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "dataset = preloading.SWEDataModule(\n",
    "    root=\"../datasets/arda/256/\",\n",
    "    test_size=0.1,\n",
    "    val_size=0,\n",
    "    past_frames=4,\n",
    "    future_frames=1,\n",
    "    partial=0.01,\n",
    "    filtering=True,\n",
    "    batch_size=4,\n",
    "    workers=4,\n",
    "    image_size=192,\n",
    "    shuffle=False,\n",
    "    dynamicity=2e-1,\n",
    "    caching=False,\n",
    "    downsampling=True\n",
    ")\n",
    "\n",
    "dataset.prepare_data()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[x] 1 areas found\n",
      "Area 0 - sequences: 59\n",
      "- - - - - - - - - - - - x x x x x - - - - "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "if th.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "\n",
    "device = th.device(dev)\n",
    "\n",
    "# -------------- Model\n",
    "print(\"[x] Loading model weights\")\n",
    "\n",
    "net = seq2seq_ConvLSTM.EncoderDecoderConvLSTM(nf=32, in_chan=4, out_chan=3)\n",
    "\n",
    "multigpu = False\n",
    "if multigpu:\n",
    "    net = nn.DataParallel(net)\n",
    "\n",
    "weights = \"../trained_models/train_135_02_09_2021_00_23_36/model.weights\"\n",
    "net.load_state_dict(\n",
    "    th.load(weights, map_location=device)\n",
    ")\n",
    "\n",
    "net = net.to(device)\n",
    "net.eval() # evaluation mode\n",
    "\n",
    "print(\"[!] Successfully loaded weights from {}\".format(weights))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[x] Loading model weights\n",
      "[!] Successfully loaded weights from ../trained_models/train_135_02_09_2021_00_23_36/model.weights\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "no_of_layers=0\n",
    "conv_layers=[]\n",
    " \n",
    "model_children=list(net.children())\n",
    "\n",
    "for child in model_children:\n",
    "  if type(child)==models.ae.ConvLSTMCell.ConvLSTMCell:\n",
    "    no_of_layers+=1\n",
    "    conv_layers.append(child.conv)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = [conv_layers[0](img)]\n",
    "for i in range(1, len(conv_layers)):\n",
    "    results.append(conv_layers[i](results[-1]))\n",
    "outputs = results"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit ('tfdeeplearning': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "interpreter": {
   "hash": "e97ca70a60c44d04c5cd941e5875e1758a8ffad8753d39a47b72b0be14848870"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}