{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Session folder: runs/train_83_30_08_2021_18_12_25\n",
      "[x] 20 areas found\n",
      "Area 0 - sequences: 245\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - x x x x x - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - x x x x x x x x x x x x x x - - - - - - - - - - - - - - - - - - - - - - x x x x x x x x x x x x - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "[5%] 31 valid sequences loaded\n",
      "Area 1 - sequences: 245\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "[10%] 0 valid sequences loaded\n",
      "Area 2 - sequences: 245\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - "
     ]
    }
   ],
   "source": [
    "from utils.data_lightning import preloading\n",
    "from utils.data_lightning import otf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mat\n",
    "import argparse\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import matplotlib as mpl\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from models.ae import seq2seq_ConvLSTM\n",
    "\n",
    "mat.use(\"Agg\") # headless mode\n",
    "\n",
    "# -------------- Functions\n",
    "\n",
    "def rss_loss(input, target):\n",
    "    return th.sum((target - input) ** 2)\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "   \n",
    "# -------------- Setting up the run\n",
    "\n",
    "num_run = len(os.listdir(\"runs/\")) + 1\n",
    "now = datetime.now()\n",
    "foldername = \"train_{}_{}\".format(num_run, now.strftime(\"%d_%m_%Y_%H_%M_%S\"))\n",
    "os.mkdir(\"runs/\" + foldername)\n",
    "weights_path = \"runs/\" + foldername + \"/model.weights\"\n",
    "\n",
    "print(\"[!] Session folder: {}\".format(\"runs/\" + foldername))\n",
    "\n",
    "writer = SummaryWriter(\"runs/\" + foldername)\n",
    "\n",
    "# -------------------------------\n",
    "plotsize = 15\n",
    "\n",
    "dataset = preloading.SWEDataModule(\n",
    "    root=\"../datasets/arda/256/\",\n",
    "    test_size=0,\n",
    "    val_size=0,\n",
    "    past_frames=4,\n",
    "    future_frames=1,\n",
    "    partial=None,\n",
    "    filtering=True,\n",
    "    batch_size=4,\n",
    "    workers=4,\n",
    "    image_size=256,\n",
    "    shuffle=False,\n",
    "    dynamicity=2e-1,\n",
    "    caching=False,\n",
    "    downsampling=False\n",
    ")\n",
    "\n",
    "dataset.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Model\n",
    "net = seq2seq_ConvLSTM.EncoderDecoderConvLSTM(nf=32, in_chan=4, out_chan=3)\n",
    "\n",
    "# Parallelism\n",
    "if th.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = th.device(dev)\n",
    "\n",
    "if th.cuda.device_count() > 1:\n",
    "  print(\"[!] Yay! Using \", th.cuda.device_count(), \"GPUs!\")\n",
    "  net = nn.DataParallel(net)\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "# ---- Training time!\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4, weight_decay=1e-5) # L2, Ridge Regression\n",
    "# L1 Lasso Regression --> https://medium.com/analytics-vidhya/understanding-regularization-with-pytorch-26a838d94058\n",
    "losses = []\n",
    "avg_losses = []\n",
    "errors = []\n",
    "test_errors = []\n",
    "print(\"\\n[!] It's training time!\")\n",
    "\n",
    "epochs = 200\n",
    "plot_graph = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction, target, threshold = 1e-2):\n",
    "\n",
    "    total = (target * prediction).cpu().detach().numpy()\n",
    "    total = np.array(total > 0).astype(int) # TP + TN + FP + FN\n",
    "\n",
    "    diff = np.abs((target - prediction).cpu().detach().numpy())\n",
    "    correct_cells = (diff < threshold).astype(int)\n",
    "    correct_cells = correct_cells*total # TP + TN\n",
    "\n",
    "    accuracy = np.sum(correct_cells)/np.sum(total)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    print(\"---- Epoch {}\".format(epoch))\n",
    "    epoch_start = time.time()\n",
    "    training_times = []\n",
    "    query_times = []\n",
    "    query_start = time.time()\n",
    "\n",
    "    iter_dataset = tqdm(enumerate(dataset.train_dataloader()), file=sys.stdout)\n",
    "    for i, batch in iter_dataset:\n",
    "        query_end = time.time()\n",
    "        query_times.append(query_end-query_start)\n",
    "\n",
    "        x, y = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "\n",
    "        # first time plot the graph\n",
    "        if not plot_graph:\n",
    "            writer.add_graph(net, x)\n",
    "            writer.close()\n",
    "            plot_graph = True\n",
    "\n",
    "        # ---- Predicting\n",
    "        start = time.time()\n",
    "        outputs = net(x, 1)  # 0 for layer index, 0 for h index\n",
    "\n",
    "        # ---- Batch Loss\n",
    "        loss = rss_loss(outputs[:, :3, 0, :, :], y[:, 0, :3, :, :])\n",
    "        acc = accuracy(outputs[:, :3, 0, :, :], y[:, 0, :3, :, :], threshold=1e-1)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        end = time.time()\n",
    "        training_times.append(end - start)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        query_start = time.time()\n",
    "\n",
    "        iter_dataset.set_postfix(\n",
    "            loss=np.mean(losses),\n",
    "            accuracy=acc,\n",
    "            fwd_time=np.mean(training_times),\n",
    "            query_time=np.mean(query_times)\n",
    "        )\n",
    "\n",
    "        writer.add_scalar('accuracy',\n",
    "                          acc,\n",
    "                          epoch*len(dataset.train_dataloader())+i)\n",
    "\n",
    "        if i % 3:\n",
    "            writer.add_scalar('avg training loss',\n",
    "                              np.mean(losses),\n",
    "                              epoch)\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    print(\"\\navg.loss {:.2f}\\ttook {:.2f} s\\tavg. inference time {:.2f} s\\tavg.query time/batch {:.2f} s\"\n",
    "          .format(epoch, np.mean(losses), epoch_end-epoch_start, np.mean(training_times), np.mean(query_times)))\n",
    "    avg_losses.append(np.mean(losses))\n",
    "\n",
    "    # checkpoint weights\n",
    "    th.save(net.state_dict(), weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "print('[!] Finished Training, storing final weights...')\n",
    "\n",
    "# Loss plot\n",
    "mpl.rcParams['text.color'] = 'k'\n",
    "\n",
    "plt.title(\"average loss\")\n",
    "plt.plot(range(len(avg_losses)), avg_losses)\n",
    "plt.savefig(\"runs/\" + foldername + \"/avg_loss.png\")\n",
    "plt.clf()\n",
    "\n",
    "print(\"Avg.training time: {}\".format(np.mean(training_times)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Tesi Laurea)",
   "language": "python",
   "name": "pycharm-b3d8d961"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
