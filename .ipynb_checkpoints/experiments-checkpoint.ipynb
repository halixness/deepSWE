{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.data_lightning import preloading\n",
    "from utils.data_lightning import otf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mat\n",
    "import matplotlib as mpl\n",
    "import argparse\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from models.ae import seq2seq_ConvLSTM\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------- Functions\n",
    "\n",
    "def rss_loss(input, target):\n",
    "    return th.sum((target - input) ** 2)\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "   \n",
    "# -------------- Setting up the run\n",
    "\n",
    "num_run = len(os.listdir(\"runs/\")) + 1\n",
    "now = datetime.now()\n",
    "foldername = \"train_{}_{}\".format(num_run, now.strftime(\"%d_%m_%Y_%H_%M_%S\"))\n",
    "os.mkdir(\"runs/\" + foldername)\n",
    "weights_path = \"runs/\" + foldername + \"/model.weights\"\n",
    "\n",
    "print(\"[!] Session folder: {}\".format(\"runs/\" + foldername))\n",
    "\n",
    "writer = SummaryWriter(\"runs/\" + foldername)\n",
    "\n",
    "# -------------------------------\n",
    "plotsize = 15\n",
    "\n",
    "dataset = preloading.SWEDataModule(\n",
    "    root=\"../datasets/arda/old_256/\",\n",
    "    test_size=0.1,\n",
    "    val_size=0,\n",
    "    past_frames=4,\n",
    "    future_frames=1,\n",
    "    partial=0.1,\n",
    "    filtering=True,\n",
    "    batch_size=4,\n",
    "    workers=4,\n",
    "    image_size=192,\n",
    "    shuffle=False,\n",
    "    dynamicity=2e-1,\n",
    "    caching=False,\n",
    "    downsampling=True\n",
    ")\n",
    "\n",
    "dataset.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[!] It's training time!\n"
     ]
    }
   ],
   "source": [
    "# ---- Model\n",
    "net = seq2seq_ConvLSTM.EncoderDecoderConvLSTM(nf=32, in_chan=4, out_chan=3)\n",
    "\n",
    "# Parallelism\n",
    "if th.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = th.device(dev)\n",
    "\n",
    "if th.cuda.device_count() > 1:\n",
    "  print(\"[!] Yay! Using \", th.cuda.device_count(), \"GPUs!\")\n",
    "  net = nn.DataParallel(net)\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "# ---- Training time!\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4, weight_decay=1e-5) # L2, Ridge Regression\n",
    "# L1 Lasso Regression --> https://medium.com/analytics-vidhya/understanding-regularization-with-pytorch-26a838d94058\n",
    "losses = []\n",
    "avg_losses = []\n",
    "errors = []\n",
    "test_errors = []\n",
    "print(\"\\n[!] It's training time!\")\n",
    "\n",
    "epochs = 200\n",
    "plot_graph = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction, target, threshold = 1e-2):\n",
    "\n",
    "    total = (target * prediction).cpu().detach().numpy()\n",
    "    total = np.array(total > 0).astype(int) # TP + TN + FP + FN\n",
    "\n",
    "    diff = np.abs((target - prediction).cpu().detach().numpy())\n",
    "    correct_cells = (diff < threshold).astype(int)\n",
    "    correct_cells = correct_cells*total # TP + TN\n",
    "\n",
    "    accuracy = np.sum(correct_cells)/np.sum(total)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epoch 0\n",
      "10it [00:00, 17.18it/s, fwd_time=0.0158, loss=6.5e+3, query_time=0.0244, test_acc=0.395, train_acc=0.395]\n",
      "\n",
      "avg.loss 0.00\ttook 6502.57 s\tavg. inference time 0.67 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 1\n",
      "10it [00:00, 18.33it/s, fwd_time=0.0142, loss=6.18e+3, query_time=0.025, test_acc=0.398, train_acc=0.408]\n",
      "\n",
      "avg.loss 1.00\ttook 6179.71 s\tavg. inference time 0.65 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 2\n",
      "10it [00:00, 18.71it/s, fwd_time=0.014, loss=5.88e+3, query_time=0.0239, test_acc=0.393, train_acc=0.394]\n",
      "\n",
      "avg.loss 2.00\ttook 5880.99 s\tavg. inference time 0.63 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 3\n",
      "10it [00:00, 18.30it/s, fwd_time=0.0145, loss=5.6e+3, query_time=0.0232, test_acc=0.389, train_acc=0.392]\n",
      "\n",
      "avg.loss 3.00\ttook 5602.27 s\tavg. inference time 0.64 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 4\n",
      "10it [00:00, 18.34it/s, fwd_time=0.0137, loss=5.34e+3, query_time=0.0258, test_acc=0.372, train_acc=0.392]\n",
      "\n",
      "avg.loss 4.00\ttook 5343.34 s\tavg. inference time 0.64 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 5\n",
      "10it [00:00, 17.95it/s, fwd_time=0.0137, loss=5.11e+3, query_time=0.0237, test_acc=0.356, train_acc=0.371]\n",
      "\n",
      "avg.loss 5.00\ttook 5107.44 s\tavg. inference time 0.65 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 6\n",
      "10it [00:00, 18.35it/s, fwd_time=0.0146, loss=4.89e+3, query_time=0.0247, test_acc=0.357, train_acc=0.375]\n",
      "\n",
      "avg.loss 6.00\ttook 4894.47 s\tavg. inference time 0.65 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 7\n",
      "10it [00:00, 16.78it/s, fwd_time=0.0158, loss=4.7e+3, query_time=0.0272, test_acc=0.362, train_acc=0.378]\n",
      "\n",
      "avg.loss 7.00\ttook 4701.49 s\tavg. inference time 0.70 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 8\n",
      "10it [00:00, 17.08it/s, fwd_time=0.0158, loss=4.53e+3, query_time=0.024, test_acc=0.37, train_acc=0.38]  \n",
      "\n",
      "avg.loss 8.00\ttook 4526.10 s\tavg. inference time 0.69 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 9\n",
      "10it [00:00, 17.23it/s, fwd_time=0.0152, loss=4.37e+3, query_time=0.0275, test_acc=0.375, train_acc=0.386]\n",
      "\n",
      "avg.loss 9.00\ttook 4365.84 s\tavg. inference time 0.70 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 10\n",
      "10it [00:00, 18.29it/s, fwd_time=0.0144, loss=4.22e+3, query_time=0.0239, test_acc=0.383, train_acc=0.39]\n",
      "\n",
      "avg.loss 10.00\ttook 4218.89 s\tavg. inference time 0.64 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 11\n",
      "10it [00:00, 15.81it/s, fwd_time=0.0194, loss=4.08e+3, query_time=0.029, test_acc=0.38, train_acc=0.393] \n",
      "\n",
      "avg.loss 11.00\ttook 4083.47 s\tavg. inference time 0.73 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 12\n",
      "10it [00:00, 19.11it/s, fwd_time=0.0154, loss=3.96e+3, query_time=0.0223, test_acc=0.376, train_acc=0.398]\n",
      "\n",
      "avg.loss 12.00\ttook 3958.30 s\tavg. inference time 0.61 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 13\n",
      "10it [00:00, 18.90it/s, fwd_time=0.0148, loss=3.84e+3, query_time=0.0217, test_acc=0.378, train_acc=0.399]\n",
      "\n",
      "avg.loss 13.00\ttook 3842.18 s\tavg. inference time 0.61 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 14\n",
      "10it [00:00, 18.52it/s, fwd_time=0.0146, loss=3.73e+3, query_time=0.0244, test_acc=0.386, train_acc=0.399]\n",
      "\n",
      "avg.loss 14.00\ttook 3734.10 s\tavg. inference time 0.64 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 15\n",
      "10it [00:00, 18.51it/s, fwd_time=0.0139, loss=3.63e+3, query_time=0.0245, test_acc=0.384, train_acc=0.407]\n",
      "\n",
      "avg.loss 15.00\ttook 3633.18 s\tavg. inference time 0.64 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 16\n",
      "10it [00:00, 18.69it/s, fwd_time=0.0139, loss=3.54e+3, query_time=0.0248, test_acc=0.381, train_acc=0.407]\n",
      "\n",
      "avg.loss 16.00\ttook 3538.68 s\tavg. inference time 0.63 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 17\n",
      "10it [00:00, 18.22it/s, fwd_time=0.0147, loss=3.45e+3, query_time=0.0269, test_acc=0.386, train_acc=0.413]\n",
      "\n",
      "avg.loss 17.00\ttook 3449.93 s\tavg. inference time 0.66 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 18\n",
      "10it [00:00, 18.69it/s, fwd_time=0.0155, loss=3.37e+3, query_time=0.0276, test_acc=0.392, train_acc=0.415]\n",
      "\n",
      "avg.loss 18.00\ttook 3366.35 s\tavg. inference time 0.67 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 19\n",
      "10it [00:00, 16.81it/s, fwd_time=0.0159, loss=3.29e+3, query_time=0.0246, test_acc=0.392, train_acc=0.42]\n",
      "\n",
      "avg.loss 19.00\ttook 3287.43 s\tavg. inference time 0.70 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 20\n",
      "10it [00:00, 18.75it/s, fwd_time=0.0142, loss=3.21e+3, query_time=0.0251, test_acc=0.398, train_acc=0.425]\n",
      "\n",
      "avg.loss 20.00\ttook 3212.72 s\tavg. inference time 0.64 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 21\n",
      "10it [00:00, 18.48it/s, fwd_time=0.0143, loss=3.14e+3, query_time=0.0244, test_acc=0.401, train_acc=0.428]\n",
      "\n",
      "avg.loss 21.00\ttook 3141.83 s\tavg. inference time 0.63 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 22\n",
      "10it [00:00, 16.59it/s, fwd_time=0.0156, loss=3.07e+3, query_time=0.0251, test_acc=0.406, train_acc=0.429]\n",
      "\n",
      "avg.loss 22.00\ttook 3074.42 s\tavg. inference time 0.70 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 23\n",
      "10it [00:00, 17.66it/s, fwd_time=0.0143, loss=3.01e+3, query_time=0.0254, test_acc=0.41, train_acc=0.425]\n",
      "\n",
      "avg.loss 23.00\ttook 3010.19 s\tavg. inference time 0.68 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 24\n",
      "10it [00:00, 16.96it/s, fwd_time=0.0158, loss=2.95e+3, query_time=0.0275, test_acc=0.414, train_acc=0.431]\n",
      "\n",
      "avg.loss 24.00\ttook 2948.89 s\tavg. inference time 0.69 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 25\n",
      "10it [00:00, 16.90it/s, fwd_time=0.0152, loss=2.89e+3, query_time=0.0248, test_acc=0.413, train_acc=0.436]\n",
      "\n",
      "avg.loss 25.00\ttook 2890.31 s\tavg. inference time 0.70 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 26\n",
      "10it [00:00, 17.41it/s, fwd_time=0.0168, loss=2.83e+3, query_time=0.0271, test_acc=0.416, train_acc=0.44]\n",
      "\n",
      "avg.loss 26.00\ttook 2834.25 s\tavg. inference time 0.68 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 27\n",
      "10it [00:00, 19.22it/s, fwd_time=0.0155, loss=2.78e+3, query_time=0.0243, test_acc=0.42, train_acc=0.447]\n",
      "\n",
      "avg.loss 27.00\ttook 2780.56 s\tavg. inference time 0.64 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 28\n",
      "10it [00:00, 18.73it/s, fwd_time=0.0154, loss=2.73e+3, query_time=0.0248, test_acc=0.419, train_acc=0.449]\n",
      "\n",
      "avg.loss 28.00\ttook 2729.09 s\tavg. inference time 0.63 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 29\n",
      "10it [00:00, 18.02it/s, fwd_time=0.0143, loss=2.68e+3, query_time=0.0254, test_acc=0.416, train_acc=0.447]\n",
      "\n",
      "avg.loss 29.00\ttook 2679.70 s\tavg. inference time 0.65 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 30\n",
      "10it [00:00, 15.95it/s, fwd_time=0.0175, loss=2.63e+3, query_time=0.0255, test_acc=0.418, train_acc=0.451]\n",
      "\n",
      "avg.loss 30.00\ttook 2632.27 s\tavg. inference time 0.73 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 31\n",
      "10it [00:00, 16.70it/s, fwd_time=0.0177, loss=2.59e+3, query_time=0.0255, test_acc=0.421, train_acc=0.445]\n",
      "\n",
      "avg.loss 31.00\ttook 2586.70 s\tavg. inference time 0.70 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 32\n",
      "10it [00:00, 18.95it/s, fwd_time=0.0156, loss=2.54e+3, query_time=0.0232, test_acc=0.429, train_acc=0.439]\n",
      "\n",
      "avg.loss 32.00\ttook 2542.88 s\tavg. inference time 0.63 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 33\n",
      "10it [00:00, 17.19it/s, fwd_time=0.015, loss=2.5e+3, query_time=0.0271, test_acc=0.43, train_acc=0.433]  \n",
      "\n",
      "avg.loss 33.00\ttook 2500.72 s\tavg. inference time 0.70 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 34\n",
      "10it [00:00, 18.18it/s, fwd_time=0.0137, loss=2.46e+3, query_time=0.0244, test_acc=0.429, train_acc=0.437]\n",
      "\n",
      "avg.loss 34.00\ttook 2460.11 s\tavg. inference time 0.66 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 35\n",
      "10it [00:00, 19.43it/s, fwd_time=0.014, loss=2.42e+3, query_time=0.0249, test_acc=0.43, train_acc=0.438] \n",
      "\n",
      "avg.loss 35.00\ttook 2420.97 s\tavg. inference time 0.63 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 36\n",
      "10it [00:00, 16.64it/s, fwd_time=0.0167, loss=2.38e+3, query_time=0.0274, test_acc=0.433, train_acc=0.439]\n",
      "\n",
      "avg.loss 36.00\ttook 2383.23 s\tavg. inference time 0.69 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 37\n",
      "10it [00:00, 16.24it/s, fwd_time=0.0172, loss=2.35e+3, query_time=0.0258, test_acc=0.437, train_acc=0.445]\n",
      "\n",
      "avg.loss 37.00\ttook 2346.81 s\tavg. inference time 0.71 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 38\n",
      "10it [00:00, 16.70it/s, fwd_time=0.0152, loss=2.31e+3, query_time=0.0272, test_acc=0.435, train_acc=0.447]\n",
      "\n",
      "avg.loss 38.00\ttook 2311.64 s\tavg. inference time 0.71 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 39\n",
      "10it [00:00, 16.65it/s, fwd_time=0.0186, loss=2.28e+3, query_time=0.0283, test_acc=0.443, train_acc=0.451]\n",
      "\n",
      "avg.loss 39.00\ttook 2277.65 s\tavg. inference time 0.72 s\tavg.query time/batch 0.02 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epoch 40\n",
      "10it [00:00, 18.90it/s, fwd_time=0.0147, loss=2.24e+3, query_time=0.0242, test_acc=0.447, train_acc=0.45]\n",
      "\n",
      "avg.loss 40.00\ttook 2244.78 s\tavg. inference time 0.63 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 41\n",
      "10it [00:00, 17.68it/s, fwd_time=0.0153, loss=2.21e+3, query_time=0.0248, test_acc=0.446, train_acc=0.455]\n",
      "\n",
      "avg.loss 41.00\ttook 2212.98 s\tavg. inference time 0.67 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 42\n",
      "10it [00:00, 17.40it/s, fwd_time=0.0159, loss=2.18e+3, query_time=0.0257, test_acc=0.45, train_acc=0.462]\n",
      "\n",
      "avg.loss 42.00\ttook 2182.19 s\tavg. inference time 0.68 s\tavg.query time/batch 0.02 s\n",
      "---- Epoch 43\n",
      "10it [00:00, 18.37it/s, fwd_time=0.0142, loss=2.15e+3, query_time=0.0238, test_acc=0.456, train_acc=0.461]\n",
      "\n",
      "avg.loss 43.00\ttook 2152.36 s\tavg. inference time 0.63 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 44\n",
      "10it [00:00, 19.31it/s, fwd_time=0.0141, loss=2.12e+3, query_time=0.0235, test_acc=0.462, train_acc=0.456]\n",
      "\n",
      "avg.loss 44.00\ttook 2123.45 s\tavg. inference time 0.62 s\tavg.query time/batch 0.01 s\n",
      "---- Epoch 45\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    print(\"---- Epoch {}\".format(epoch))\n",
    "    epoch_start = time.time()\n",
    "    training_times = []\n",
    "    query_times = []\n",
    "    query_start = time.time()\n",
    "\n",
    "    iter_dataset = tqdm(enumerate(dataset.train_dataloader()), file=sys.stdout)\n",
    "    for i, batch in iter_dataset:\n",
    "        query_end = time.time()\n",
    "        query_times.append(query_end-query_start)\n",
    "\n",
    "        x, y = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "\n",
    "        # first time plot the graph\n",
    "        if not plot_graph:\n",
    "            writer.add_graph(net, x)\n",
    "            writer.close()\n",
    "            plot_graph = True\n",
    "\n",
    "        # ---- Predicting\n",
    "        start = time.time()\n",
    "        outputs = net(x, 1)  # 0 for layer index, 0 for h index\n",
    "\n",
    "        # ---- Batch Loss\n",
    "        loss = rss_loss(outputs[:, :3, 0, :, :], y[:, 0, :3, :, :])\n",
    "        acc = accuracy(outputs[:, :3, 0, :, :], y[:, 0, :3, :, :], threshold=1e-1)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        end = time.time()\n",
    "        training_times.append(end - start)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        query_start = time.time()\n",
    "        \n",
    "        # ----- Testing\n",
    "        size = len(dataset.datasets[1] )\n",
    "        random_test_batch = dataset.datasets[1][random.randint(0, size-1)]\n",
    "        x_test, y_test = batch\n",
    "        x_test = x_test.float().to(device)\n",
    "        y_test = y_test.float().to(device)\n",
    "        test_outputs = net(x_test, 1) \n",
    "        test_acc = accuracy(test_outputs[:, :3, 0, :, :], y_test[:, 0, :3, :, :], threshold=1e-1)\n",
    "\n",
    "        writer.add_scalar('test_accuracy',\n",
    "                      test_acc,\n",
    "                      epoch*len(dataset.train_dataloader())+i)\n",
    "\n",
    "        writer.add_scalar('train_accuracy',\n",
    "                          acc,\n",
    "                          epoch*len(dataset.train_dataloader())+i)\n",
    "\n",
    "        # Plot values\n",
    "        if i % 3:\n",
    "            writer.add_scalar('avg training loss',\n",
    "                              np.mean(losses),\n",
    "                              epoch)\n",
    "            \n",
    "        iter_dataset.set_postfix(\n",
    "            loss=np.mean(losses),\n",
    "            train_acc=acc,\n",
    "            test_acc=test_acc,\n",
    "            fwd_time=np.mean(training_times),\n",
    "            query_time=np.mean(query_times)\n",
    "        )\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    print(\"\\navg.loss {:.2f}\\ttook {:.2f} s\\tavg. inference time {:.2f} s\\tavg.query time/batch {:.2f} s\"\n",
    "          .format(epoch, np.mean(losses), epoch_end-epoch_start, np.mean(training_times), np.mean(query_times)))\n",
    "    avg_losses.append(np.mean(losses))\n",
    "\n",
    "    # checkpoint weights\n",
    "    th.save(net.state_dict(), weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "print('[!] Finished Training, storing final weights...')\n",
    "\n",
    "# Loss plot\n",
    "mpl.rcParams['text.color'] = 'k'\n",
    "\n",
    "plt.title(\"average loss\")\n",
    "plt.plot(range(len(avg_losses)), avg_losses)\n",
    "plt.savefig(\"runs/\" + foldername + \"/avg_loss.png\")\n",
    "plt.clf()\n",
    "\n",
    "print(\"Avg.training time: {}\".format(np.mean(training_times)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Tesi Laurea)",
   "language": "python",
   "name": "pycharm-b3d8d961"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
