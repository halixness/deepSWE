{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tqdm \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "import keras.layers.advanced_activations as advanced_activations\n",
    "import keras.activations as activations\n",
    "import keras.layers as layers\n",
    "from keras.layers import Dense, concatenate\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, Conv3D, Conv3DTranspose\n",
    "from tensorflow.keras.layers import MaxPooling3D\n",
    "from tensorflow.keras.layers import AveragePooling3D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout, Add\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.accuracy = []\n",
    "        \n",
    "    def loss_plot(self):\n",
    "        plt.plot(range(len(self.losses)), self.losses)\n",
    "        \n",
    "    def accuracy_plot(self):\n",
    "        plt.plot(range(len(self.accuracy)), self.accuracy)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.accuracy.append(logs.get('accuracy'))\n",
    "\n",
    "        #print(\"Loss: {}, Val_loss: {}, Accuracy: {}\".format(logs.get('loss'), logs.get('loss'), logs.get('accuracy')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_frames = 10\n",
    "future_frames = 4\n",
    "step = 7\n",
    "img_size = 256\n",
    "train_size = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../datasets/baganza/\"\n",
    "minis =  os.listdir(root)\n",
    "\n",
    "# indices is aligned with that\n",
    "dataset_filenames = [x for x in sorted(minis) if x.startswith(\"mini-\")]\n",
    "\n",
    "# handpicked intervals\n",
    "dataset_intervals = [(300, 800), (300, 800), (390, 900), (300, 800), (390, 900), (390, 850), (350, 800), (350, 900)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# non ho finito di filtrarli\n",
    "# assert(len(dataset_filenames) == len(dataset_intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  dataset_partitions\n",
    "#       dataset_1\n",
    "#            partition: [start_window_id]\n",
    "#            labels: [ids: id([target_window])]\n",
    "#\n",
    "#       dataset_2\n",
    "#            partition: [start_window_id]\n",
    "#            labels: [ids: id([target_window])]\n",
    "#\n",
    "#       (...)\n",
    "\n",
    "train_dataset_partitions = []\n",
    "test_dataset_partitions = []\n",
    "\n",
    "partition = dict()\n",
    "labels = dict()\n",
    "\n",
    "# Applica intervalli di interesse\n",
    "for di in dataset_intervals:\n",
    "\n",
    "    # FIn ai frame che interessano\n",
    "    size = di[1] - di[0] - past_frames - future_frames\n",
    "    train_len = int(size * train_size)\n",
    "    \n",
    "    partition_raw = []\n",
    "    \n",
    "    # Sliding window per sequenze\n",
    "    x = 0\n",
    "    for i in range(0, size, step):\n",
    "        partition_raw.append(\"id-{}\".format(x))\n",
    "        labels[\"id-{}\".format(x)] = list(range(i + (past_frames * step), i + (past_frames * step) + (future_frames * step), step))\n",
    "        x += 1\n",
    "        \n",
    "    partition_t = partition_raw[:train_len]\n",
    "    partition_v = partition_raw[train_len:] \n",
    "    \n",
    "    # lista di (partition, labels)\n",
    "    train_dataset_partitions.append((partition_t, labels))\n",
    "    test_dataset_partitions.append((partition_v, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataset_partitions, batch_size=16, input_dim=(past_frames, img_size, img_size),  output_dim=(future_frames, img_size, img_size), n_channels=1, shuffle=True):\n",
    "        'Initialization'\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.dataset_partitions = dataset_partitions\n",
    "\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def get_total_frames(self):\n",
    "        total_frames = 0\n",
    "    \n",
    "        for p in self.dataset_partitions:\n",
    "            total_frames += len(p[0])\n",
    "        \n",
    "        return total_frames\n",
    "    \n",
    "    def get_total_batches(self):\n",
    "        batches = 0\n",
    "        \n",
    "        # sum of batches per dataset\n",
    "        for p in self.dataset_partitions:\n",
    "            batches += int(np.floor(len(p[0]) / self.batch_size))\n",
    "            \n",
    "        return batches\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        \n",
    "        return self.get_total_batches()\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "\n",
    "        # Seleziona un intervallo di frames in base all'indice batch\n",
    "        indexes = self.indexes[ index * self.batch_size : (index + 1) * self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        \n",
    "        total_frames = self.get_total_frames()\n",
    "            \n",
    "        # Shuffle di tutti gli id frames globali (attinge da pi√π datasets a random)\n",
    "        self.indexes = np.arange(total_frames)\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        \n",
    "    def get_dataset_id(self, index):\n",
    "        curmax = 0\n",
    "        \n",
    "        # Get index of which dataset it belongs to\n",
    "        for i, p in enumerate(self.dataset_partitions):\n",
    "            curmax += len(p[0])\n",
    "            if index < curmax:\n",
    "                return i\n",
    "        return -1\n",
    "    \n",
    "    def get_local_id(self, global_id, dataset_id):\n",
    "    \n",
    "        # remove lens of all the previous ones\n",
    "        for i in range(dataset_id):\n",
    "            global_id -= len(self.dataset_partitions[i][0])\n",
    "        \n",
    "        return global_id\n",
    "        \n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        \n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X = np.empty((self.batch_size, *self.input_dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, *self.output_dim, self.n_channels))\n",
    "\n",
    "        # Generate data for i IDs\n",
    "        # id: 0 +\n",
    "        for i, global_id in enumerate(list_IDs_temp):\n",
    "            \n",
    "            dataset_id = self.get_dataset_id(global_id)\n",
    "            local_id = self.get_local_id(global_id, dataset_id)\n",
    "            \n",
    "            # Rimuovo valori estremi e seleziono frames nella curva\n",
    "            start, end = dataset_intervals[dataset_id]\n",
    "            src = np.load(root + dataset_filenames[i])[start:end]\n",
    "            src[src > 10e5] = 0\n",
    "            \n",
    "            # partition[id] -> start position for train window\n",
    "            # ID\n",
    "            r = src[local_id: local_id + past_frames]\n",
    "            X[i,] = r.reshape(r.shape[0], r.shape[1], r.shape[2], 1)\n",
    "\n",
    "            # labels[id] -> list of frame indices for predict window\n",
    "            r = src[self.dataset_partitions[dataset_id][1][\"id-{}\".format(local_id)]]\n",
    "            y[i] = r.reshape(r.shape[0], r.shape[1], r.shape[2], 1)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = DataGenerator(train_dataset_partitions, batch_size=4)\n",
    "test_d = DataGenerator(test_dataset_partitions, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 490 is out of bounds for axis 0 with size 460",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-320b0384a14b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# First train frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Last target frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-fa419b2b46c1>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Generate data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__data_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-fa419b2b46c1>\u001b[0m in \u001b[0;36m__data_generation\u001b[0;34m(self, list_IDs_temp)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m# labels[id] -> list of frame indices for predict window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_partitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id-{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 490 is out of bounds for axis 0 with size 460"
     ]
    }
   ],
   "source": [
    "# First train frame\n",
    "plt.matshow(train_d.__getitem__(1)[0][0][0].reshape(256,256))\n",
    "print(np.mean(train_d.__getitem__(1)[0][0][0]))\n",
    "\n",
    "# Last target frame\n",
    "plt.matshow(train_d.__getitem__(1)[1][0][3].reshape(256,256))\n",
    "print(np.mean(train_d.__getitem__(1)[1][0][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_shape = (10, 256, 256, 1)\n",
    "inputs = Input(shape = sample_shape)\n",
    "\n",
    "x = inputs\n",
    "\n",
    "x = Conv3D(32, kernel_size=(3, 3, 3), padding='same', activation='relu', kernel_initializer='he_uniform', input_shape=sample_shape)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling3D(pool_size=(2, 2, 2))(x)\n",
    "\n",
    "x = Conv3D(64, kernel_size=(3, 3, 3), padding='same', activation='relu', kernel_initializer='he_uniform', input_shape=sample_shape)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling3D(pool_size=(2, 2, 2))(x)\n",
    "\n",
    "x = Conv3DTranspose(filters=64, kernel_size=(3,3,3), strides=(2,2,2), padding=\"same\",activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Conv3DTranspose(filters=32, kernel_size=(3,3,3), strides=(1,2,2), padding=\"same\",activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Conv3DTranspose(filters=1, kernel_size=(3,3,3), strides=(1,1,1), padding=\"same\",activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "model = Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = Adam(learning_rate = 1e-3), loss = \"mean_squared_error\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x = train_d,\n",
    "    validation_data = test_d,\n",
    "    use_multiprocessing = True,\n",
    "    workers = 6,\n",
    "    epochs = 1,\n",
    "    callbacks=[history]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5))\n",
    "fig.suptitle('frame 1 - frame 11 pred - frame 11 true')\n",
    "\n",
    "ax1.matshow(test_d.__getitem__(10)[0][0][0].reshape(256,256))\n",
    "ax2.matshow(model.predict(test_d.__getitem__(10)[0])[0][0].reshape(256,256))\n",
    "ax3.matshow(test_d.__getitem__(10)[1][0][0].reshape(256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.accuracy_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loss_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
