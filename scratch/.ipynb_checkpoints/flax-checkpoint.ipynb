{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda-10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp            # JAX NumPy\n",
    "\n",
    "from flax import linen as nn        # The Linen API\n",
    "from flax import optim              # Optimizers\n",
    "\n",
    "import numpy as np                 # Ordinary NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "  # equivale a forward()\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = x.reshape((x.shape[0], -1))  # flatten\n",
    "    x = nn.Dense(features=256)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=10)(x)\n",
    "    x = nn.log_softmax(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, labels):\n",
    "  one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
    "  return -jnp.mean(jnp.sum(one_hot_labels * logits, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass network params to the optimizer \n",
    "def create_optimizer(params, learning_rate, beta):\n",
    "  optimizer_def = optim.Momentum(learning_rate=learning_rate, beta=beta)\n",
    "  \n",
    "  # wraps the model parameters for future update\n",
    "  optimizer = optimizer_def.create(params)\n",
    "  return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_params(key):\n",
    "  init_shape = jnp.ones((1, 28, 28, 1), jnp.float32)\n",
    "  initial_params = CNN().init(key, init_shape)['params']\n",
    "  return initial_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logits, labels):\n",
    "  loss = cross_entropy_loss(logits, labels)\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "  metrics = {\n",
    "      'loss': loss,\n",
    "      'accuracy': accuracy\n",
    "  }\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "  ds_builder = tfds.builder('mnist')\n",
    "  ds_builder.download_and_prepare()\n",
    "  # Split into training/test sets\n",
    "  train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "  test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "  # Convert to floating-points\n",
    "  train_ds['image'] = jnp.float32(train_ds['image']) / 255.0\n",
    "  test_ds['image'] = jnp.float32(test_ds['image']) / 255.0\n",
    "  return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(optimizer, batch):\n",
    "    \n",
    "  def loss_fn(params):\n",
    "    logits = CNN().apply({'params': params}, batch['image'])\n",
    "    loss = cross_entropy_loss(logits, batch['label'])\n",
    "    return loss, logits\n",
    "  \n",
    "  # gradient and value\n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  \n",
    "  # predictions\n",
    "  (_, logits), grad = grad_fn(optimizer.target)\n",
    "  \n",
    "  # apply gradient descend\n",
    "  optimizer = optimizer.apply_gradient(grad)\n",
    "  \n",
    "  # get accuracy\n",
    "  metrics = compute_metrics(logits, batch['label'])\n",
    "  return optimizer, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "  # use the computed params from optimizer\n",
    "  logits = CNN().apply({'params': params}, batch['image'])\n",
    "  return compute_metrics(logits, batch['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(optimizer, train_ds, batch_size, epoch, rng):\n",
    "  train_ds_size = len(train_ds['image'])\n",
    "  steps_per_epoch = train_ds_size // batch_size\n",
    "    \n",
    "  perms = jax.random.permutation(rng, len(train_ds['image']))\n",
    "  perms = perms[:steps_per_epoch * batch_size]  # Skip an incomplete batch\n",
    "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "  batch_metrics = []\n",
    "\n",
    "  for perm in perms:\n",
    "    batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
    "    \n",
    "    # passes batch and optimizer\n",
    "    optimizer, metrics = train_step(optimizer, batch)\n",
    "    batch_metrics.append(metrics)\n",
    "\n",
    "  training_batch_metrics = jax.device_get(batch_metrics)\n",
    "  training_epoch_metrics = {\n",
    "      k: np.mean([metrics[k] for metrics in training_batch_metrics])\n",
    "      for k in training_batch_metrics[0]}\n",
    "\n",
    "  print('Training - epoch: %d, loss: %.4f, accuracy: %.2f' % (epoch, training_epoch_metrics['loss'], training_epoch_metrics['accuracy'] * 100))\n",
    "\n",
    "  return optimizer, training_epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_ds):\n",
    "  metrics = eval_step(model, test_ds)    # Evalue the model on the test set\n",
    "  metrics = jax.device_get(metrics)\n",
    "  eval_summary = jax.tree_map(lambda x: x.item(), metrics)\n",
    "  return eval_summary['loss'], eval_summary['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_initial_params(init_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "beta = 0.9\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "optimizer = create_optimizer(params, learning_rate=learning_rate, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f30bac114f76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_rng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Run an optimization step over a training batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_rng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;31m# Evaluate on the test set after each training epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "  # Use a separate PRNG key to permute image data during shuffling\n",
    "  rng, input_rng = jax.random.split(rng)\n",
    "  \n",
    "  # Run an optimization step over a training batch\n",
    "  optimizer, train_metrics = train_epoch(optimizer, train_ds, batch_size, epoch, input_rng)\n",
    "  \n",
    "  # Evaluate on the test set after each training epoch \n",
    "  test_loss, test_accuracy = eval_model(optimizer.target, test_ds)\n",
    "  print('Testing - epoch: %d, loss: %.2f, accuracy: %.2f' % (epoch, test_loss, test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
